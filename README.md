# ReviewSites

## Idea
 
I had this idea that there's are somewhat subtle - though observable - tendencies for domain specific review/rating sites. For example, on rateyourmusic.com - a music focused review site - certain genres are usually better rated. And a 4/5 on RYM doesn't necessarily mean the same as a 4/5 on any other site. 

The most active (and accessible) commnunities that I know of are RYM for music, GoodReads (GR) for books, and Letterboxd (LB) for film.

## Methodology

My initial thoughts are to compare the top X all-time rated works for each site. Possible comparisons would include: normalized rating, position all-time, rating distance to other works (e.g. next highest, next lowest, #1), NLP on reviews.

The methods would be largely exploratory at first. I would also benefit from using a control though this may be tricky. It's hard to say what a control would be in this situation, IMDB? iTunes? RottenTomatoes? It might not be feasible.

## Hypotheses

1. Genre has a significant effect on rating across all sites.

2. Number of reviews/ratings for all-time lists decline exponentially, but overall rating declines linearly.

3. Popularity of works will not be a significant factor of rating -> popular works, even some acclaimed ones, will not necessarily have good ratings, and underground works can have very good ratings.

